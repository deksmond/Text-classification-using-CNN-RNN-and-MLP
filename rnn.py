# -*- coding: utf-8 -*-
"""rnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylUM3A5BQU6PkMuO21suxSPv5PwBpMe0

# **Recurrent Neural Network (RNN)**

Debug data loading to check dataset and labels
"""

# Before the training loop: Print the first few padded training sequences and their labels
print("First 5 Padded Training Sequences:")
print(X_train_padded[:5])
print("First 5 One-Hot Encoded Training Labels:")
print(y_train_one_hot[:5])

# RNN Configuration parameters
class TRNNConfig(object):

    embedding_dim = 128  # Word vector dimension
    seq_length = max_length  # Sequence length (Taken from preprocessing)
    num_classes = num_classes # Number of classes (Taken from preprocessing)
    vocab_size = len(word_index) + 1  # Vocabulary size (Taken from tokenizer)

    hidden_dim = 256        # Hidden layer neurons

    dropout_keep_prob = 0.5  # Dropout keep probability
    learning_rate = 1e-4   # Learning Rate

    batch_size = 64         # Training size per batch
    num_epochs = 10          # Number of training epochs

    print_per_batch = 100    # print every number of batches
    save_per_batch = 10      # save per number of batches


# RNN Model build
class TextRNN(tf.keras.Model):

    def __init__(self, config):
        super(TextRNN, self).__init__()
        self.config = config

        self.embedding = tf.keras.layers.Embedding(self.config.vocab_size, self.config.embedding_dim)
        self.bi_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.config.hidden_dim, return_sequences=False))
        self.fc1 = tf.keras.layers.Dense(self.config.hidden_dim * 2, activation='relu') # Adjusted for bidirectional
        self.dropout = tf.keras.layers.Dropout(self.config.dropout_keep_prob)
        self.fc2 = tf.keras.layers.Dense(self.config.num_classes, activation='softmax')

    def call(self, inputs):
        embedding_inputs = self.embedding(inputs)
        bi_gru = self.bi_gru(embedding_inputs)
        fc = self.fc1(bi_gru)
        fc = self.dropout(fc)
        logits = self.fc2(fc)
        return logits

"""**Recurrent Neural Network (RNN) training and evaluation**

This section trains the RNN model. It sets up the RNN model, optimizer, and loss function. It defines RNN-specific training (train_step_rnn with gradient clipping) and evaluation functions. It reuses the preprocessed and padded datasets. The training loop is structurally similar to the CNN loop, but it uses the RNN-specific training and evaluation steps. The key RNN-specific aspect in the training process is the gradient clipping in train_step_rnn, which is important for stabilizing RNN training.
"""

# Model Training
config_rnn = TRNNConfig()
model_rnn = TextRNN(config_rnn)

optimizer_rnn = tf.keras.optimizers.Adam(config_rnn.learning_rate)
loss_func_rnn = tf.keras.losses.CategoricalCrossentropy()

def train_step_rnn(inputs, labels):
    with tf.GradientTape() as tape:
        predictions = model_rnn(inputs)
        loss = loss_func_rnn(labels, predictions)
    gradients = tape.gradient(loss, model_rnn.trainable_variables)
    clipped_gradients = [tf.clip_by_norm(g, 1.0) for g in gradients] # Clip gradients
    optimizer_rnn.apply_gradients(zip(clipped_gradients, model_rnn.trainable_variables))
    accuracy = np.mean(np.argmax(labels, axis=1) == np.argmax(predictions, axis=1))
    return loss, accuracy

def evaluate_model_rnn(inputs, labels):
    predictions = model_rnn(inputs)
    loss = loss_func_rnn(labels, predictions)
    accuracy = np.mean(np.argmax(labels, axis=1) == np.argmax(predictions, axis=1))
    return loss, accuracy

epochs_rnn = config_rnn.num_epochs
batch_size_rnn = config_rnn.batch_size
train_dataset_rnn = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train_one_hot)).shuffle(len(X_train_padded)).batch(batch_size_rnn)
val_dataset_rnn = tf.data.Dataset.from_tensor_slices((X_val_padded, y_val_one_hot)).batch(batch_size_rnn)
test_dataset_rnn = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test_one_hot)).batch(batch_size_rnn)

# Initialize lists to store losses and accuracies for RNN
train_losses_rnn = []
val_losses_rnn = []
train_accuracies_rnn = []
val_accuracies_rnn = []


print("\nTraining the RNN model...")
for epoch in range(epochs_rnn):
    epoch_loss_avg = tf.keras.metrics.Mean()
    epoch_accuracy = tf.keras.metrics.Mean()
    for batch, (inputs, labels) in enumerate(train_dataset_rnn):
        loss, accuracy = train_step_rnn(inputs, labels)
        epoch_loss_avg.update_state(loss)
        epoch_accuracy.update_state(accuracy)
        if batch % config_rnn.print_per_batch == 0:
            print(f"Epoch {epoch+1}/{epochs_rnn} Batch {batch} Loss: {epoch_loss_avg.result():.4f} Accuracy: {epoch_accuracy.result():.4f}")

    # Store epoch losses and accuracies for RNN
    train_losses_rnn.append(epoch_loss_avg.result().numpy())
    train_accuracies_rnn.append(epoch_accuracy.result().numpy())

    val_loss_avg = tf.keras.metrics.Mean()
    val_accuracy = tf.keras.metrics.Mean()
    for inputs, labels in val_dataset_rnn:
        loss, accuracy = evaluate_model_rnn(inputs, labels)
        val_loss_avg.update_state(loss)
        val_accuracy.update_state(accuracy)

    # Store epoch losses and accuracies for RNN
    val_losses_rnn.append(val_loss_avg.result().numpy())
    val_accuracies_rnn.append(val_accuracy.result().numpy())

    print(f"Epoch {epoch+1}/{epochs_rnn} Validation Loss: {val_loss_avg.result():.4f} Validation Accuracy: {val_accuracy.result():.4f}")

model_rnn.summary()

print("\nEvaluating the RNN model on the test set...")
test_loss_avg = tf.keras.metrics.Mean()
test_accuracy = tf.keras.metrics.Mean()
for inputs, labels in test_dataset_rnn:
    loss, accuracy = evaluate_model_rnn(inputs, labels)
    test_loss_avg.update_state(loss)
    test_accuracy.update_state(accuracy)
print(f"Test Loss: {test_loss_avg.result():.4f} Test Accuracy: {test_accuracy.result():.4f}")

"""Training and validation loss"""

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses_rnn, label='Training Loss')
plt.plot(val_losses_rnn, label='Validation Loss')
plt.title('RNN Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

"""Training and validation accuracy"""

plt.subplot(1, 2, 2)
plt.plot(train_accuracies_rnn, label='Training Accuracy')
plt.plot(val_accuracies_rnn, label='Validation Accuracy')
plt.title('RNN Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Confusion matrix"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_pred_rnn = model_rnn.predict(X_test_padded)
y_pred_rnn_classes = np.argmax(y_pred_rnn, axis=1)
cm_rnn = confusion_matrix(np.argmax(y_test_one_hot, axis=1), y_pred_rnn_classes)
disp_rnn = ConfusionMatrixDisplay(confusion_matrix=cm_rnn, display_labels=label_encoder.classes_)
disp_rnn.plot(cmap=plt.cm.Blues)
plt.title('RNN Confusion Matrix')
plt.show()